{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kb_eqxZWoLC",
        "outputId": "de43ac1d-fe6c-4e93-cb7d-89b59ba66aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set (X_train, Y_train) shapes: (23899, 15) (23899, 4)\n",
            "Validation set (X_val, Y_val) shapes: (2656, 15) (2656, 4)\n",
            "Test set (X_test, Y_test) shapes: (2951, 15) (2951, 4)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/Final_eskom_vector_grid_health.csv')\n",
        "data = pd.DataFrame(data)\n",
        "data['Time'] = pd.to_datetime(data['DateTime'])\n",
        "data.set_index('Time', inplace=True)\n",
        "\n",
        "\n",
        "Y_columns = [\n",
        "    'ILS Usage',\n",
        "    'MLR', 'IOS Excl ILS and MLR',\n",
        "     'Total UCLF+OCLF'\n",
        "]\n",
        "Y = data[Y_columns].values\n",
        "X = data.drop(columns=Y_columns, axis=1)\n",
        "X.drop(['DateTime'], axis =1, inplace = True)\n",
        "X = X.values\n",
        "\n",
        "X_train_temp, X_test, Y_train_temp, Y_test = train_test_split(X, Y, test_size=0.10, shuffle=False, random_state=42)\n",
        "\n",
        "# Then, split the temporary test set into validation (50%) and final test (50%)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_temp, Y_train_temp, test_size=0.10, shuffle=False, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Training set (X_train, Y_train) shapes:\", X_train.shape, Y_train.shape)\n",
        "print(\"Validation set (X_val, Y_val) shapes:\", X_val.shape, Y_val.shape)\n",
        "print(\"Test set (X_test, Y_test) shapes:\", X_test.shape, Y_test.shape)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_normalised = scaler.fit_transform(X_train)\n",
        "X_test_normalised = scaler.transform(X_test)\n",
        "X_val_norm = scaler.transform(X_val)\n",
        "\n",
        "Y_train_normalised = scaler.fit_transform(Y_train)\n",
        "Y_test_normalised = scaler.transform(Y_test)\n",
        "Y_val_norm = scaler.transform(Y_val)\n",
        "\n",
        "\n",
        "timesteps = 24*7\n",
        "window_length = timesteps\n",
        "batch_size = 16\n",
        "num_features = 15\n",
        "train_generator = TimeseriesGenerator(X_train_normalised, Y_train_normalised,length=window_length, batch_size=batch_size)\n",
        "test_generator = TimeseriesGenerator(X_test_normalised, Y_test_normalised,length=window_length, batch_size=batch_size)\n",
        "val_generator = TimeseriesGenerator(X_val_norm, Y_val_norm, length=window_length, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final test on architecture type 2Layers-1B-1L vs 3Layers-2B-1L vs 2Layers-2L"
      ],
      "metadata": {
        "id": "sKDYQYD2Z4Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "num_layers = 16\n",
        "patience = 8\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=True, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "JdssnZd8bUcn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2 layers 2 LSTM\n",
        " #num epochs without improvement\n",
        "\n",
        "model_simple = tf.keras.Sequential([\n",
        "  layers.LSTM(16, input_shape = (window_length, 15), return_sequences=True, activation='tanh'),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.LSTM(16, input_shape = (window_length, 15)),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(4, activation='linear')])\n",
        "\n",
        "model_simple.build((None, timesteps, 15))\n",
        "model_simple.summary()\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min', verbose=True, restore_best_weights=True)\n",
        "\n",
        "\n",
        "model_simple.compile(loss=tf.keras.losses.Huber(delta = 0.3),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[tf.keras.losses.Huber()])\n",
        "history_simple = model_simple.fit(train_generator, epochs=epochs, validation_data= val_generator,  callbacks=[early_stopping], verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqwR_3MFZ_11",
        "outputId": "d4cca403-cc1e-40e0-fed3-c07fc35bdc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 168, 16)           2048      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 168, 16)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 68        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4228 (16.52 KB)\n",
            "Trainable params: 4228 (16.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1484/1484 [==============================] - 213s 138ms/step - loss: 0.0812 - huber_loss: 0.1972 - val_loss: 0.0699 - val_huber_loss: 0.1643\n",
            "Epoch 2/50\n",
            "1484/1484 [==============================] - 202s 136ms/step - loss: 0.0713 - huber_loss: 0.1760 - val_loss: 0.0738 - val_huber_loss: 0.1786\n",
            "Epoch 3/50\n",
            "1484/1484 [==============================] - 198s 133ms/step - loss: 0.0681 - huber_loss: 0.1692 - val_loss: 0.0737 - val_huber_loss: 0.1769\n",
            "Epoch 4/50\n",
            "1484/1484 [==============================] - 198s 133ms/step - loss: 0.0667 - huber_loss: 0.1661 - val_loss: 0.0710 - val_huber_loss: 0.1715\n",
            "Epoch 5/50\n",
            " 259/1484 [====>.........................] - ETA: 2:49 - loss: 0.0691 - huber_loss: 0.1765"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2 layers 1 Birectional 1 LSTM\n",
        "\n",
        "modelH03 = tf.keras.Sequential([\n",
        "  layers.Bidirectional(\n",
        "  layers.LSTM(16, input_shape = (window_length, 15), return_sequences=True, activation='tanh')),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.LSTM(16,  return_sequences=False),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(4, activation='linear')])\n",
        "\n",
        "modelH03.build((None, timesteps, 15))\n",
        "modelH03.summary()\n",
        "\n",
        "modelH03.compile(loss=tf.keras.losses.Huber(delta=0.3),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[tf.keras.losses.Huber()])\n",
        "\n",
        "history_2Layers_simple = modelH03.fit(train_generator, epochs=epochs, validation_data= val_generator,  callbacks=[early_stopping], verbose=1)\n"
      ],
      "metadata": {
        "id": "_1vFqpEea4kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3 layers 2 Bidirectional 1 LSTM\n",
        "model_decay_3 = tf.keras.Sequential([\n",
        "  layers.Bidirectional(\n",
        "  layers.LSTM(16, input_shape = (window_length, 15), return_sequences=True, activation='tanh')),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Bidirectional(layers.LSTM(16,  return_sequences=True)),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.LSTM(16,  return_sequences=False),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(4, activation='linear')])\n",
        "\n",
        "model_decay_3.build((None, timesteps, 15))\n",
        "model_decay_3.summary()\n",
        "\n",
        "model_decay_3.compile(loss=tf.keras.losses.Huber(delta=0.3),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[tf.keras.losses.Huber()])\n",
        "model_history_decay_3 = model_decay_3.fit(train_generator, epochs=epochs, validation_data= val_generator,  callbacks=[early_stopping], verbose=1)"
      ],
      "metadata": {
        "id": "TSh90D_2cJat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dicts = [history_simple.history, history_2Layers_simple.history, model_history_decay_3.history]\n",
        "\n",
        "# Define labels for each model\n",
        "model_labels = ['Model 1', 'Model 2', 'Model 3']\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Loop through each model's history\n",
        "for i, history_dict in enumerate(history_dicts):\n",
        "    error = history_dict['huber_loss']\n",
        "    val_error = history_dict['val_huber_loss']\n",
        "    loss = history_dict['loss']\n",
        "    val_loss = history_dict['val_loss']\n",
        "\n",
        "    epochs = range(1, len(error) + 1)\n",
        "\n",
        "    # Plot training loss with dots\n",
        "    plt.plot(epochs, loss, 'o-', label=f'{model_labels[i]} - Training loss')\n",
        "\n",
        "    # Plot validation loss with lines\n",
        "    plt.plot(epochs, val_loss, '-', label=f'{model_labels[i]} - Validation loss')\n",
        "\n",
        "plt.title('Training and validation loss for Multiple Models')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "no7CmaVKcyfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6MiqtcsdKUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}